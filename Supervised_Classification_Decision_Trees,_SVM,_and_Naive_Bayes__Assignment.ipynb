{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q.1. What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "i5uaDW6He30p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Information Gain  is a measure used in Decision Trees (e.g., ID3, C4.5) to decide which feature to split on at each step of the tree-building process.\n",
        "\n",
        "   how information gain is used in decisions trees:\n",
        "\n",
        "   1. feature selection at each node\n",
        "    \n",
        "    * Compute IG for each candidate feature.\n",
        "\n",
        "    * Choose the feature that gives the highest Information Gain.\n",
        "\n",
        "    * This feature maximally reduces uncertainty about the target.\n",
        "\n",
        "  2. create more \"pure\" child nodes\n",
        "   \n",
        "    * More homogeneous\n",
        "\n",
        "    * Closer to a single class\n",
        "\n",
        "    * Easier to classify\n",
        "\n",
        "  3. Helps control tree structure\n",
        "   \n",
        "    * High IG → good split\n",
        "\n",
        "    * Low IG → poor split\n",
        "This affects the depth and accuracy of the final tree."
      ],
      "metadata": {
        "id": "ADqXgTpltoHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "kXD2OVZFjItc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.\n",
        "   \n",
        "   | Aspect                | Gini Impurity                 | Entropy                |\n",
        "| --------------------- | ----------------------------- | ---------------------- |\n",
        "| Used in               | CART                          | ID3, C4.5              |\n",
        "| Formula               | (1 - \\sum p_i^2)              | (-\\sum p_i \\log_2 p_i) |\n",
        "| Interpretation        | Misclassification probability | Measure of uncertainty |\n",
        "| Computation           | Faster                        | Slightly slower (log)  |\n",
        "| Splitting tendency    | Favors pure class splits      | Favors balanced splits |\n",
        "| Output range (binary) | 0 → 0.5                       | 0 → 1                  |\n"
      ],
      "metadata": {
        "id": "gQpBwAcltwhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3. What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "_Z4L1mMNoV5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Pre-pruning prevents the decision tree from splitting a node if the split does not provide sufficient improvement according to some criterion.\n",
        "Instead of growing a full tree and pruning afterward, the algorithm blocks certain splits during training."
      ],
      "metadata": {
        "id": "rbnvD7ETt8VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "HjkCAhiNuC07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split dataset into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree using Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# 5. Optional: Accuracy\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"\\nModel Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaO7Ys6muRUh",
        "outputId": "7510cb11-79c3-4350-9a84-bae12e86cb75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5. What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "w2fUies_u7vP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. A Support Vector Machine finds the best separating boundary (hyperplane) between classes by maximizing the margin—the distance between the boundary and the closest data points (called support vectors)."
      ],
      "metadata": {
        "id": "HVlX2kwavAeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6. What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "JMFsDRlnvU1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The Kernel Trick is a technique used in Support Vector Machines (SVMs) that allows the model to learn non-linear decision boundaries without explicitly transforming the data into higher dimensions."
      ],
      "metadata": {
        "id": "Spujy6g4vuJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "eQZPw2lxvywI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Standardize features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# 5. Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 6. Print results\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5vJeNsqv56M",
        "outputId": "7ce33370-2593-4256-97be-eeb069a31289"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
      ],
      "metadata": {
        "id": "GZ-z-PpowES1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. A Naïve Bayes classifier is a probabilistic machine learning model based on Bayes’ Theorem, commonly used for classification tasks such as spam detection, text classification, sentiment analysis, and more.\n",
        "\n",
        "  Why is it called navie:\n",
        "   \n",
        "   It is called naïve because it makes a strong and unrealistic assumption"
      ],
      "metadata": {
        "id": "pmprpKW8wKp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes."
      ],
      "metadata": {
        "id": "zpTFJQaFwQ5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. 1.Gaussian Naïve Bayes\n",
        "\n",
        "   Continuous features\n",
        " Data that follows a normal (Gaussian) distribution\n",
        "\n",
        "       2. Multinomial Naïve Bayes\n",
        "\n",
        "       Count-based features\n",
        "       Text classification (word frequencies or counts)\n",
        "\n",
        "       3. Bernoulli Naïve Bayes\n",
        "         \n",
        "         Binary (0/1) features\n",
        "         Whether a word appears or not (not how many times)"
      ],
      "metadata": {
        "id": "NakPHcbIwbUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "25b5FKqiwi6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naïve Bayes classifier:\", accuracy)\n",
        "\n",
        "# Optional: Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD4kiM57w0V3",
        "outputId": "38fb414c-03d5-465f-c950-eee5efd7f970"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes classifier: 0.9415204678362573\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    }
  ]
}